Hi I'm Dan giraffe ski and Chris manning and I are very happy to welcome you to our course on natural language processing this is a particularly exciting time to be working on natural language processing the vast amount of data on the web and social media have made it possible to build fantastic new applications. Let's look at one of them. Question answering you may know that IBM's Watson won the jeopardy challenge on 2/16/2011. And answering questions like William Wilkinson's book inspired this author's most famous novel and you may know that the answer is Bram Stoker who famously wrote. Dracula. Another important task is information extra. For example imagine that I have the following email from my colleague Chris about scheduling a meeting. We'd like software to automatically notice that there are dates like. Tomorrow times like 101130 and a room like gates 159. Extract those information create a new calendar entry and then populate the calendar with this kind of structured information with the event date start in and. For calendar program in modern I'm email and calendar programs are capable of doing this from text. Another application of this kind of information extraction involve sentiment analysis imagine that you're interested in cameras in your reading a lot of reviews of cameras on the web so here's a bunch of a bunch of reviews. We'd like to automatically determined from the reviews that what people care about and cameras are particular attributes of their buying a camera they want to know if has good zoom or affordability or size and weight to an automatic determine those attributes. And then we'd like to automatically for any particular attribute determine how the reviewers felt about those attributes for example if a reviewer said nice and compact to carry that's a positive sentiment but here's another positive example but if but a phrase like flimsy is a negative sentiment so we'd like to automatically detect for each sentence what the sentiment is and then aggregate preach features force a presume from fort abilities when Mike decided this camera reviews we like the flash. But they weren't so happy about the ease of use we might measure the positive and negative sentiment. About each attribute and then aggregate those. Machine translation is another important new application and missing translation can be fully automatic so for example we might have a source sentencing Chinese and here's Stanford's phrasal anti system translating that into English but empty can also be used to help human translators so here we might have an Arabic text. And the human translator translated into English might need some help from the M. T. system for example but a collection of possible next words that the anti system can build automatically and help human translator. Let's look at the state of the art in language technology. Like every field and appease divided up into specialties and subspecialties a number of these problems are pretty close to solved so for example spam detection well it's very hard to completely detect spam in our email boxes we don't have yeah I 99 percent spam and that's because spam detection is a relatively an easy classification task. And a couple of important component tasks part of speech tagging in named entity tagging will talk about those a leader in the course and those work at pretty high accuracy is gonna get 97 percent accuracy in part of speech. Time and we see how that's important for parsing. In other tasks we're making good progress. Not as commercial not as completely solved but there are systems out there that are that are being used so we talked about sentiment analysis the task of deciding thumbs up or thumbs down on a sentence or product. Component technologies like word sense disambiguation deciding for talking about a rodent or a computer mouse when people talk about mouse is in a search. And we'll talk about parsing which is a good enough now to be used in lots of applications and machine translation usable on the web. A number of applications however are still quite hard so for example at answering hard questions like how effective is this medicine in treating that disease by looking at the Weber by summarizing information we know was quite hard. Similarly when we made some progress on deciding that the sentence X. Y. Z. company acquired ABC company yesterday mean something similar to EBC has been taken over by X. Y. Z. the general problem of detecting that 2 phrases or sentences mean the same thing the paraphrase task. Still quite hard. Even harder is the task of summarization reading a number of let's say news articles that say the Dow Jones is up or the SNP 500 is jumped and housing prices rose and aggregating that to give a user information like in summary the economy is good. And finally one of the hardest tasks in natural language processing carrying on a complete human machine communication in dialogue. So here's a simple example asking about what movie is playing when and buy movie tickets and you can get applications that do that today. But the general problem of understanding everything the user might ask for and have returning a sensible response is quite difficult. Why is natural language processing so difficult. One cute example are the kinds of ambiguity problems that are called crash blossom so ambiguities any case where a surface form might have multiple interpretations. A crash blossom is the name for a kind of headline that has 2 meanings. And the ambiguity causes a humorous interpretation. So reading this first headline violinist linked to J. L. crash blossoms. You might think that the main verb is linked. And a violinist is being linked to what. He's been linked to Japan Airlines crash blossoms well what a crash blossoms. Well this headline gave the name to this phenomenon because the actual interpretation that the headline writer intended the main verb was blossoms who does the blossoming of violinist. And this fact about being willing to jail crash was a modifier violinist. Similar kinds of syntactic ambiguities. So here teacher strikes idol kids the writer intended the main verb to be idle the strikes. Because the kids to be idle but of course the humorous interpretation is that the teacher is striking strike is the verb and we have a teacher. Striking. Idol kids. Another important kind of ambiguity is word sense ambiguity. So in our third example red tape holds up new bridges the writer intended holds up to mean something like delay. Collette since one of holds up. But the damn amusing interpretation is the second sense of holds up which we might write down is to support. And now we have the interpretation that literal red tape as opposed to bureaucratic red tape is actually supporting a bridge. And we can see lots of other kinds of am ambiguities in these actual headlines. Now it turns out that it's not just amusing headlines that have ambiguity ambiguity is pervasive throughout natural language text let's look at a sensible non ambiguous looking headline from The New York Times. So the headline with shortened it here a bit and is fed raises interest rates lessons unambiguous. We have a verb here out little parse tree raises what gets raised. And now I'm phrase alright a little to announce your interest rates. And we'll have a verb phrase. So raising interest rates and then we'll have the fed. Make a little noun phrase and they will say this is a sentence that has announced rates fed and a verb phrase raises and what its raises interest rates of this is called a phrase structure parse. Talk about that. Leader in the course free structure. So we can also write a dependency parcel we see the head for raises has an argument which is fed and has another dependent witches rates and Han rates has another itself has a dependent interest so we can see the main verb is raising. Another interpretation of the very same sentence when that people don't see but that parser see right away is that it's not raises that's the main verb the sentence but interest. Somebody interests something. And then that's something that gets interested is rates. And what is interesting these rates well. It's fed raises raises by the fed so the complete different settings with a different reputation that something is interesting the rates whatever that could mean and it seems unlikely interpretation for people but of course for a parser this is a perfectly reasonable interpretation we have to learn how to rule out in fact the sentence can get even more difficult this is the actual headline with some somewhat longer so we had fed raises interest rates half a percent here we could imagine that rates is the verb. And now we have what is reading fed raises interest the interest in federal raises. Our rating half a percent so we might have a. And it depends you structure like this. So again interest. Rates. The raises are what do the interesting enough as a modifier raises so whether with our free structure parse or dependency parse and even more so as we add more words will get more and more ambiguity that have to be solved in order to build a parse for each sentence. Now the format of the course you're gonna happen in video quizzes and most lectures will include a little quiz